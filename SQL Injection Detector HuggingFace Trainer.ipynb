{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nvidia-ml-py3 lime torchvision\n",
    "!pip install wandb evaluate\n",
    "!pip install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MobileBertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import json\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "from transformers import AdamW, Adafactor, get_linear_schedule_with_warmup\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    GPT2ForSequenceClassification,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from pynvml import (\n",
    "    nvmlInit,\n",
    "    nvmlDeviceGetHandleByIndex,\n",
    "    nvmlDeviceGetMemoryInfo,\n",
    "    nvmlDeviceGetCount,\n",
    "    nvmlDeviceGetName,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    deviceCount = nvmlDeviceGetCount()\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvmlDeviceGetHandleByIndex(i)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(\"Device\", i, \":\", nvmlDeviceGetName(handle))\n",
    "        print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"data/Modified_SQL_Dataset-trainer.csv\", split=\"train\")\n",
    "print(dataset)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "raw_train = dataset[\"train\"]\n",
    "raw_test = dataset[\"test\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mobilebert-uncased')\n",
    "batch_size = 32\n",
    "logging_steps = len(raw_train) // batch_size\n",
    "lr = 1e-4\n",
    "\n",
    "default_args = {\n",
    "    \"report_to\": \"none\", # disable WANDB\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": batch_size,\n",
    "    \"per_device_eval_batch_size\":batch_size,\n",
    "    \"output_dir\": \"train_model/\",\n",
    "    \"learning_rate\":lr,\n",
    "    \"weight_decay\":0.01,\n",
    "    \"eval_steps\":100,\n",
    "    \"logging_steps\":100,\n",
    "    #\"save_steps\":100,\n",
    "    \"optim\": \"adamw_torch\", #\"adamw_torch\", #\"adafactor\",\n",
    "    \"warmup_steps\":70,\n",
    "    \"save_strategy\": \"no\", #no epoch steps\n",
    "    \"do_predict\": False\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "train = raw_train.map(lambda x: tokenizer(x[\"Query\"], truncation=True, max_length=512), batched=True, remove_columns=[\"Query\"])\n",
    "test = raw_test.map(lambda x: tokenizer(x[\"Query\"], truncation=True, max_length=512), batched=True, remove_columns=[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import os\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "model = MobileBertForSequenceClassification.from_pretrained('google/mobilebert-uncased')\n",
    "training_args = TrainingArguments(**default_args)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    #optimizers=(optimizer, lr_scheduler)\n",
    ")\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
