{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "# !pip uninstall datasets==2.14.4 evaluate transformers[sentencepiece] -y\n",
    "# !pip uninstall accelerate -y\n",
    "# !pip uninstall PyArrow==12.0.1 -y\n",
    "# !pip uninstall Pandas==2.0.3 -y\n",
    "# #!pip uninstall gradio\n",
    "\n",
    "!pip install datasets evaluate transformers\n",
    "!pip install accelerate\n",
    "!pip install PyArrow\n",
    "!pip install Pandas\n",
    "!pip install gradio\n",
    "!pip install nltk\n",
    "!pip install rouge\n",
    "!pip install rouge_score\n",
    "\n",
    "\n",
    "#!pip install  git+https://github.com/NVIDIA/apex.git@0da3ffb\n",
    "#!pip uninstall   apex -y\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "# try:\n",
    "#     # Remove the directory and all its contents\n",
    "#     shutil.rmtree(\"./apex\")\n",
    "#     print(f'Successfully deleted')\n",
    "# except Exception as e:\n",
    "#     print(f'An error occurred while deleting {path}: {e}')\n",
    "# print(os.curdir)\n",
    "# !git clone https://github.com/NVIDIA/apex\n",
    "%cd ./apex\n",
    "print(os.curdir)\n",
    "!pip install  --force-reinstall -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MobileBertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import json\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "from transformers import AdamW, Adafactor\n",
    "#from apex import amp\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Use it like this at the beginning of your code\n",
    "set_seed(42)\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "#from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "#from nltk.translate.meteor_score import meteor_score\n",
    "#from rouge import Rouge\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import time\n",
    "import datetime \n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from transformers import Adafactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SQLInjectionPipeline:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        logging.info(f\"Configuration: {self.config}\")\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        self.labels = []\n",
    "        self.train_dataloader = None\n",
    "        self.val_dataloader = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "\n",
    "        self._device_setup()\n",
    "        self._initialize_metrics()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config['model_name'])\n",
    "        self.EPOCHS = self.config['EPOCHS']\n",
    "\n",
    "    def initialize_model(self):\n",
    "        logging.info(\"Initializing model...\")\n",
    "        print(\"Initializing model...\")\n",
    "        \n",
    "        if self.config['ft_model']:\n",
    "            self.model = torch.load(self.config['ft_model'])\n",
    "            print(f\"Loaded fine-tuned model: {self.config['ft_model']}\")\n",
    "        elif self.config['ft_model_sd']:\n",
    "            self.model = MobileBertForSequenceClassification.from_pretrained(self.config['model_name'])\n",
    "            self.model.load_state_dict(torch.load(self.config['ft_model_sd']))\n",
    "            print(f\"Loaded fine-tuned model state_dict: {self.config['ft_model_sd']}\")\n",
    "        else:\n",
    "            self.model = MobileBertForSequenceClassification.from_pretrained(\n",
    "                self.config['model_name']\n",
    "            )\n",
    "            print(f\"Loaded model : {self.config['model_name']}\")\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        #print(self.model.config)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            logging.info(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "        if self.config['optimizer'] == 'Adafactor':\n",
    "            self.optimizer = Adafactor(self.model.parameters(), **self.config['adafactor_config'])\n",
    "            print(\"OPTIMIZER Adafactor initialized!\")\n",
    "        elif self.config['optimizer'] == 'AdamW':\n",
    "            self.optimizer = AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=self.config['lr'],\n",
    "                weight_decay=self.config['weight_decay']\n",
    "            )\n",
    "            print(\"OPTIMIZER AdamW initialized!\")\n",
    "                \n",
    "           # Calculate the total steps\n",
    "        total_steps = len(self.train_dataloader) * self.config['EPOCHS']\n",
    "        \n",
    "        if self.config['scheduler'] == 'linear' and self.config['optimizer'] != 'Adafactor':\n",
    "            # Create the learning rate scheduler.\n",
    "            self.scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer, \n",
    "                num_warmup_steps=self.config['warmup_steps'],  # You can define warmup_steps in your config\n",
    "                num_training_steps=total_steps\n",
    "                )\n",
    "            print(\"Scheduler initialized\")\n",
    "        \n",
    "        logging.info(\"Model initialized\")\n",
    "        print(\"Model initialized\")\n",
    "\n",
    "    def _initialize_metrics(self):\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.roc_auc_scores = []\n",
    "\n",
    "\n",
    "    def download_and_tokenize_data(self):\n",
    "        print(\"download_and_tokenize_data started...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        def tokenize_data(query, label):\n",
    "            # This part is common to both JSON and CSV\n",
    "            inputs = self.tokenizer(query, padding=False, truncation=True,max_length=512, return_tensors=None)\n",
    "            self.input_ids.append(torch.tensor(inputs['input_ids']))\n",
    "            self.attention_masks.append(torch.tensor(inputs['attention_mask']))\n",
    "            #label =  self.tokenizer(label, padding=False, truncation=True, return_tensors=None)['input_ids']\n",
    "            self.labels.append(torch.tensor(label))        \n",
    "            #self.labels.append(label)        \n",
    "\n",
    "        data_file = self.config['data_file']\n",
    "        data_format = self.config['data_format']\n",
    "\n",
    "        print(self.config['data_keys'][0])\n",
    "        print(self.config['data_keys'][1])\n",
    "        if data_format == 'csv':\n",
    "            with open(data_file, 'r') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    query = row[self.config['data_keys'][0]]\n",
    "                    label = row[self.config['data_keys'][1]]\n",
    "                    tokenize_data(query,int(label))\n",
    "                \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"download_and_tokenize_data completed. Time:{elapsed:.2f}s\")\n",
    "        \n",
    "    def prepare_and_convert(self):\n",
    "        print(\"prepare_and_convert started\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Dynamic padding collate function\n",
    "        def collate_fn(batch):\n",
    "            input_ids, attention_masks, labels = zip(*batch)\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "            attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "            #labels = pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "            labels = torch.tensor(labels)\n",
    "            return input_ids, attention_masks, labels\n",
    "\n",
    "        # Zip the lists together to form your dataset\n",
    "        dataset = list(zip(self.input_ids, self.attention_masks, self.labels))\n",
    "\n",
    "        # Split dataset into training and validation sets\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "        # Create DataLoader for training and validation sets\n",
    "        self.train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32, collate_fn=collate_fn)\n",
    "        self.val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"prepare_and_convert completed. Time: {elapsed:.2f}s\")\n",
    "        \n",
    "    def _device_setup(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        logging.info(\"Device setup complete\")\n",
    "        print(\"Device setup complete\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        logging.info(\"Evaluation started...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        all_pred_ids = []\n",
    "        all_label_ids = []\n",
    "\n",
    "        for batch in self.val_dataloader:\n",
    "            b_input_ids = batch[0].to(self.device)\n",
    "            b_attention_masks = batch[1].to(self.device)\n",
    "            b_labels = batch[2].to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            pred_ids = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_pred_ids.extend(pred_ids.cpu().numpy())\n",
    "            all_label_ids.extend(b_labels.cpu().numpy())\n",
    "\n",
    "        # Calculate precision, recall, F1-score and ROC/AUC\n",
    "        precision = precision_score(all_label_ids, all_pred_ids, average='weighted')\n",
    "        recall = recall_score(all_label_ids, all_pred_ids, average='weighted')\n",
    "        f1 = f1_score(all_label_ids, all_pred_ids, average='weighted')\n",
    "        roc_auc = roc_auc_score(all_label_ids, all_pred_ids, multi_class='ovr', average='weighted')\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, ROC/AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Evaluation completed. Time: {elapsed:.2f}s\")\n",
    "\n",
    "        logging.info(\"Evaluation completed\")\n",
    "\n",
    "        return precision, recall, f1, roc_auc\n",
    "    \n",
    "    def train(self):\n",
    "        from torch.nn import CrossEntropyLoss\n",
    "        loss_function = CrossEntropyLoss()        \n",
    "        \n",
    "        training_results = []\n",
    "        start = datetime.datetime.now()\n",
    "        logging.info(f\"Training started. AMP enabled: {self.config['use_amp']}. At {start}\")\n",
    "        print(f\"Training started. AMP enabled: {self.config['use_amp']}. At {start}\")\n",
    "\n",
    "        if self.config['use_amp']:\n",
    "            scaler = GradScaler()\n",
    "\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            step = 0\n",
    "            start_time = time.time()\n",
    "            for batch in self.train_dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                b_input_ids = batch[0].to(self.device)\n",
    "                b_attention_masks = batch[1].to(self.device)\n",
    "                b_labels = batch[2].to(self.device)\n",
    "\n",
    "                if self.config['use_amp']:\n",
    "                    with autocast():\n",
    "                        outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "                        loss = outputs.loss\n",
    "                    scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    #outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "                    #loss = outputs.loss\n",
    "                    outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_masks)\n",
    "                    logits = outputs.logits\n",
    "                    loss = loss_function(logits, b_labels)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    if self.config['scheduler'] and self.config['optimizer'] != 'Adafactor':\n",
    "                        self.scheduler.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                step += 1\n",
    "\n",
    "                if step % int(self.config['log_batches']) == 0 and step > 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"Epoch {epoch+1}/{self.EPOCHS}, Step {step}/{len(self.train_dataloader)}, Loss: {loss.item():.4f}, Time/batch: {elapsed:.2f}s\") \n",
    "                    start_time = time.time()\n",
    "\n",
    "            avg_train_loss = total_loss / len(self.train_dataloader)\n",
    "            print(f\"Epoch {epoch+1} finished. Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Evaluate after each epoch\n",
    "            precision, recall, f1, roc_auc = self.evaluate()\n",
    "            training_results.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_train_loss': avg_train_loss,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'roc_auc': roc_auc\n",
    "            })\n",
    "\n",
    "            # Save the model after each epoch\n",
    "            ep_model_loc = f\"{self.config['save_loc']}/{self.config['model']}_{self.config['version']}_model_epoch_{epoch}.pt\"\n",
    "            torch.save(self.model, ep_model_loc)\n",
    "\n",
    "        end = datetime.datetime.now()\n",
    "        logging.info(f\"Training ended. AMP enabled: {self.config['use_amp']}. At {end}\")\n",
    "        print(f\"Training ended. AMP enabled: {self.config['use_amp']}. At {end}\")\n",
    "\n",
    "        return training_results\n",
    "\n",
    "    def setup_and_train(self):\n",
    "        self.download_and_tokenize_data()\n",
    "        \n",
    "        self.prepare_and_convert()\n",
    "        \n",
    "        self.initialize_model()\n",
    "        \n",
    "        results = self.train()\n",
    "        return results;\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    'model': 'bertmobile-sql-inject-detector',\n",
    "    'model_name': 'google/mobilebert-uncased',\n",
    "    'ft_model': '',\n",
    "    'ft_model_sd': '',\n",
    "    'optimizer': 'Adafactor' ,#'AdamW',\n",
    "    'dynamic_padding': True,\n",
    "    'use_amp': False,\n",
    "    'use_autocast': False,    \n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'EPOCHS': 5,\n",
    "    'data_file': 'data/Modified_SQL_Dataset.csv',\n",
    "    'data_format': 'csv',\n",
    "    'data_keys': ['Query', 'Label'],\n",
    "    'save_loc': \"modelmobile_v1\",\n",
    "    'log_batches': 50,\n",
    "    'version': 'v1',\n",
    "    'warmup_steps': 94,\n",
    "    'scheduler': 'linear',\n",
    "    'adafactor_config': {\n",
    "        'lr': 1e-3,\n",
    "        'eps': (1e-30, 1e-3),\n",
    "        'clip_threshold': 1.0,\n",
    "        'decay_rate': -0.8,\n",
    "        'beta1': None,\n",
    "        'weight_decay': 0.0,\n",
    "        'relative_step': False,\n",
    "        'scale_parameter': False,\n",
    "        'warmup_init': False\n",
    "    },\n",
    "}\n",
    "\n",
    "pipeline = SQLInjectionPipeline(config);\n",
    "results = pipeline.setup_and_train()\n",
    "\n",
    "for r in results:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
